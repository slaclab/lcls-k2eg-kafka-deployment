---
# Source: consul/templates/server-disruptionbudget.yaml
# PodDisruptionBudget to prevent degrading the server cluster through
# voluntary cluster changes.
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: consul
      release: "consul"
      component: server
---
# Source: consul/templates/server-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
---
# Source: consul/templates/server-config-configmap.yaml
# StatefulSet to run the actual Consul server cluster.
apiVersion: v1
kind: ConfigMap
metadata:
  name: consul-server-config
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
data:
  server.json: |
    {
      "bind_addr": "0.0.0.0",
      "bootstrap_expect": 3,
      "client_addr": "0.0.0.0",
      "connect": {
        "enabled": true
      },
      "datacenter": "dc1",
      "data_dir": "/consul/data",
      "enable_debug": false,
      "domain": "consul",
      "limits": {
        "request_limits": {
          "mode": "disabled",
          "read_rate": -1,
          "write_rate": -1
        }
      },
      "ports": {
        "grpc": 8502,
        "grpc_tls": -1,
        "serf_lan": 8301
      },
      "recursors": [],
      "retry_join": ["consul-server.kafka.svc:8301"],
      "server": true,
      "leave_on_terminate": true,
      "autopilot": {
        "min_quorum": 2,
        "disable_upgrade_migration": true
      }
    }
  ui-config.json: |-
    {
      "ui_config": {
        "enabled": true
      }
    }
  central-config.json: |-
    {
      "enable_central_service_config": true
    }
---
# Source: consul/templates/server-tmp-extra-config-configmap.yaml
# ConfigMap that is used as a temporary landing spot so that the container command
# in the server-stateful set where it needs to be transformed.  ConfigMaps create
# read only volumes so it needs to be copied and transformed to the extra-config
# emptyDir volume where all final extra cofngi lives for use in consul.  (locality-init
# also writes to extra-config volume.)
apiVersion: v1
kind: ConfigMap
metadata:
  name: consul-server-tmp-extra-config
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
data:
  extra-from-values.json: |-
    {}
---
# Source: consul/templates/server-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs:
  - get
---
# Source: consul/templates/server-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: consul-server
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: consul-server
subjects:
- kind: ServiceAccount
  name: consul-server
  namespace: kafka
---
# Source: consul/templates/server-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
rules: []
---
# Source: consul/templates/server-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: consul-server
subjects:
  - kind: ServiceAccount
    name: consul-server
---
# Source: consul/templates/dns-service.yaml
# Service for Consul DNS.
apiVersion: v1
kind: Service
metadata:
  name: consul-dns
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: dns
spec:
  type: ClusterIP
  ports:
    - name: dns-tcp
      port: 53
      protocol: "TCP"
      targetPort: dns-tcp
    - name: dns-udp
      port: 53
      protocol: "UDP"
      targetPort: dns-udp
  selector:
    app: consul
    release: "consul"
    hasDNS: "true"
---
# Source: consul/templates/server-service.yaml
# Headless service for Consul server DNS entries. This service should only
# point to Consul servers. For access to an agent, one should assume that
# the agent is installed locally on the node and the NODE_IP should be used.
# If the node can't run a Consul agent, then this service can be used to
# communicate directly to a server agent.
apiVersion: v1
kind: Service
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
  annotations:
spec:
  clusterIP: None
  # We want the servers to become available even if they're not ready
  # since this DNS is also used for join operations.
  publishNotReadyAddresses: true
  ports:
    - name: http
      port: 8500
      targetPort: 8500
    - name: grpc
      port: 8502
      targetPort: 8502
    - name: serflan-tcp
      protocol: "TCP"
      port: 8301
      targetPort: 8301
    - name: serflan-udp
      protocol: "UDP"
      port: 8301
      targetPort: 8301
    - name: serfwan-tcp
      protocol: "TCP"
      port: 8302
      targetPort: 8302
    - name: serfwan-udp
      protocol: "UDP"
      port: 8302
      targetPort: 8302
    - name: server
      port: 8300
      targetPort: 8300
    - name: dns-tcp
      protocol: "TCP"
      port: 8600
      targetPort: dns-tcp
    - name: dns-udp
      protocol: "UDP"
      port: 8600
      targetPort: dns-udp
  selector:
    app: consul
    release: "consul"
    component: server
---
# Source: consul/templates/ui-service.yaml
# UI Service for Consul Server
apiVersion: v1
kind: Service
metadata:
  name: consul-ui
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: ui
spec:
  selector:
    app: consul
    release: "consul"
    component: server
  ports:
    - name: http
      port: 80
      targetPort: 8500
---
# Source: consul/templates/server-statefulset.yaml
# StatefulSet to run the actual Consul server cluster.
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: consul-server
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: server
spec:
  serviceName: consul-server
  podManagementPolicy: Parallel
  replicas: 3
  selector:
    matchLabels:
      app: consul
      chart: consul-helm
      release: consul
      component: server
      hasDNS: "true"
  template:
    metadata:
      labels:
        app: consul
        chart: consul-helm
        release: consul
        component: server
        hasDNS: "true"
      annotations:
        "consul.hashicorp.com/connect-inject": "false"
        "consul.hashicorp.com/mesh-inject": "false"
        "consul.hashicorp.com/config-checksum": b15e2e1a9aee9c8e3bdc982b1dc9f09695dec8ddbf631a292bb3ee98b898e21b
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: consul
                  release: "consul"
                  component: server
              topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 30
      serviceAccountName: consul-server
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 100
      volumes:
        - name: tmp
          emptyDir: {}
        - name: config
          configMap:
            name: consul-server-config
        - name: extra-config
          emptyDir: {}
        - name: tmp-extra-config
          configMap:
            name: consul-server-tmp-extra-config
      initContainers:
      - name: locality-init
        image: hashicorp/consul-k8s-control-plane:1.6.3
        

        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        command:
          - "/bin/sh"
          - "-ec"
          - |
            exec consul-k8s-control-plane fetch-server-region -node-name "$NODE_NAME" -output-file /consul/extra-config/locality.json
        volumeMounts:
          - name: extra-config
            mountPath: /consul/extra-config
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
          runAsUser: 100
      containers:
        - name: consul
          image: "hashicorp/consul:1.20.5"
          

          imagePullPolicy: 
          env:
            - name: ADVERTISE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: CONSUL_DISABLE_PERM_MGMT
              value: "true"
            - name: GOSSIP_KEY
              valueFrom:
                secretKeyRef:
                  name: consul-gossip-encryption-key
                  key: key
            
          command:
            - "/bin/sh"
            - "-ec"
            - |

              cp /consul/tmp/extra-config/extra-from-values.json /consul/extra-config/extra-from-values.json
              [ -n "${HOST_IP}" ] && sed -Ei "s|HOST_IP|${HOST_IP?}|g" /consul/extra-config/extra-from-values.json
              [ -n "${POD_IP}" ] && sed -Ei "s|POD_IP|${POD_IP?}|g" /consul/extra-config/extra-from-values.json
              [ -n "${HOSTNAME}" ] && sed -Ei "s|HOSTNAME|${HOSTNAME?}|g" /consul/extra-config/extra-from-values.json

              exec /usr/local/bin/docker-entrypoint.sh consul agent \
                -advertise="${ADVERTISE_IP}" \
                -config-dir=/consul/config \
                -encrypt="${GOSSIP_KEY}" \
                -config-dir=/consul/extra-config \
          volumeMounts:
            - name: data-kafka
              mountPath: /consul/data
            - name: config
              mountPath: /consul/config
            - name: extra-config
              mountPath: /consul/extra-config
            - name: tmp-extra-config
              mountPath: /consul/tmp/extra-config
            - name: tmp
              mountPath: /tmp
              readOnly: false
          ports:
            - name: http
              containerPort: 8500
            - name: grpc
              containerPort: 8502
              protocol: "TCP"
            - name: serflan-tcp
              containerPort: 8301
              protocol: "TCP"
            - name: serflan-udp
              containerPort: 8301
              protocol: "UDP"
            - name: serfwan-tcp
              containerPort: 8302
              protocol: "TCP"
            - name: serfwan-udp
              containerPort: 8302
              protocol: "UDP"
            - name: server
              containerPort: 8300
            - name: dns-tcp
              containerPort: 8600
              protocol: "TCP"
            - name: dns-udp
              containerPort: 8600
              protocol: "UDP"
          readinessProbe:
            # NOTE(mitchellh): when our HTTP status endpoints support the
            # proper status codes, we should switch to that. This is temporary.
            exec:
              command:
                - "/bin/sh"
                - "-ec"
                - |
                  curl http://127.0.0.1:8500/v1/status/leader \
                  2>/dev/null | grep -E '".+"'
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 5Gi
            requests:
              cpu: "1"
              memory: 5Gi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
              - ALL
              add:
              - NET_BIND_SERVICE
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            runAsUser: 100
  volumeClaimTemplates:
    - metadata:
        name: data-kafka
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
---
# Source: consul/templates/gossip-encryption-autogenerate-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: consul-gossip-encryption-autogenerate
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: gossip-encryption-autogenerate
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
---
# Source: consul/templates/gossip-encryption-autogenerate-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: consul-gossip-encryption-autogenerate
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: gossip-encryption-autogenerate
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
rules:
- apiGroups: [""]
  resources:
    - secrets
  verbs:
    - create
    - get
---
# Source: consul/templates/gossip-encryption-autogenerate-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: consul-gossip-encryption-autogenerate
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: gossip-encryption-autogenerate
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: consul-gossip-encryption-autogenerate
subjects:
- kind: ServiceAccount
  name: consul-gossip-encryption-autogenerate
---
# Source: consul/templates/tests/test-runner.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "consul-test"
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: consul-test
      image: "hashicorp/consul:1.20.5"
      

      env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: CONSUL_HTTP_ADDR
          value: http://$(HOST_IP):8500
      command:
        - "/bin/sh"
        - "-ec"
        - |
            consul members | tee members.txt
            if [ $(grep -c consul-server members.txt) != $(grep consul-server members.txt | grep -c alive) ]
            then
              echo "Failed because not all consul servers are available"
              exit 1
            fi

  restartPolicy: Never
---
# Source: consul/templates/gossip-encryption-autogenerate-job.yaml
# automatically generate encryption key for gossip protocol and save it in Kubernetes secret
apiVersion: batch/v1
kind: Job
metadata:
  name: consul-gossip-encryption-autogenerate
  namespace: kafka
  labels:
    app: consul
    chart: consul-helm
    heritage: Helm
    release: consul
    component: gossip-encryption-autogenerate
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "1"
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      name: consul-gossip-encryption-autogenerate
      labels:
        app: consul
        chart: consul-helm
        release: consul
        component: gossip-encryption-autogenerate
      annotations:
        "consul.hashicorp.com/connect-inject": "false"
        "consul.hashicorp.com/mesh-inject": "false"
    spec:
      restartPolicy: Never
      serviceAccountName: consul-gossip-encryption-autogenerate
      securityContext:
        runAsNonRoot: true
        runAsGroup: 1000 
        runAsUser: 100 
        fsGroup: 1000
      containers:
        - name: gossip-encryption-autogen
          image: "hashicorp/consul-k8s-control-plane:1.6.3"
          

          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
              - ALL
              add:
              - NET_BIND_SERVICE
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
            runAsUser: 100
          command:
            - "/bin/sh"
            - "-ec"
            - |
              exec consul-k8s-control-plane gossip-encryption-autogenerate \
                -namespace=kafka \
                -secret-name=consul-gossip-encryption-key \
                -secret-key="key" \
                -log-level=info \
                -log-json=false
          resources:
            requests:
              memory: "50Mi"
              cpu: "50m"
            limits:
              memory: "50Mi"
              cpu: "50m"
